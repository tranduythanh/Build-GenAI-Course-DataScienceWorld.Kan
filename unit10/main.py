# story_rag_chatbot_graph.py

import os
from pathlib import Path
from dotenv import load_dotenv
from llama_index.core import Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding
import gradio as gr
from story_indexer import (
    load_and_parse_documents,
    build_or_load_graph_index,
)

# ------------------ Load .env ------------------
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
assert OPENAI_API_KEY, "â— Vui lÃ²ng cÃ i Ä‘áº·t OPENAI_API_KEY trong file .env cá»§a báº¡n."

# ------------------ CONFIG ------------------
OPENAI_MODEL = "gpt-4.1-mini"
EMBEDDING_MODEL = "text-embedding-3-small"

# Khá»Ÿi táº¡o LLM vÃ  embedding model
llm = OpenAI(model=OPENAI_MODEL)
embed_model = OpenAIEmbedding(model=EMBEDDING_MODEL)
Settings.llm = llm
Settings.embed_model = embed_model

# ------------------ EXAMPLES ------------------
EXAMPLES = [
    "Ká»ƒ tÃ³m táº¯t cÃ¢u chuyá»‡n",
    "Ai lÃ  nhÃ¢n váº­t chÃ­nh trong truyá»‡n?",
    "Káº¿t thÃºc cá»§a cÃ¢u chuyá»‡n nhÆ° tháº¿ nÃ o?",
    "CÃ³ nhá»¯ng nhÃ¢n váº­t nÃ o trong truyá»‡n?",
    "Ná»™i dung chÃ­nh cá»§a cÃ¢u chuyá»‡n lÃ  gÃ¬?"
]



PROMPT = '''
Dá»±a trÃªn cÃ¡c Ä‘oáº¡n vÄƒn sau Ä‘Ã¢y, hÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng má»™t cÃ¡ch ngáº¯n gá»n vÃ  chÃ­nh xÃ¡c.
    
CÃ¢u há»i: {user_input}

CÃ¡c Ä‘oáº¡n vÄƒn liÃªn quan:
{context}

CÃ¢u tráº£ lá»i:
'''


def process_query(user_input, story_retrievers, selected_story, llm):
    if not user_input.strip():
        return "Vui lÃ²ng nháº­p cÃ¢u há»i cá»§a báº¡n."
    
    # Náº¿u chá»n "Táº¥t cáº£ truyá»‡n", tÃ¬m kiáº¿m trÃªn táº¥t cáº£
    if selected_story == "Táº¥t cáº£ truyá»‡n":
        all_nodes = []
        for story_name, retriever in story_retrievers.items():
            nodes = retriever.retrieve(user_input)
            all_nodes.extend(nodes)
    else:
        # TÃ¬m kiáº¿m chá»‰ trong truyá»‡n Ä‘Æ°á»£c chá»n
        if selected_story not in story_retrievers:
            return "âŒ KhÃ´ng tÃ¬m tháº¥y truyá»‡n Ä‘Æ°á»£c chá»n."
        nodes = story_retrievers[selected_story].retrieve(user_input)
        all_nodes = nodes
        
    if not all_nodes:
        return "âŒ KhÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan."
        
    # Sáº¯p xáº¿p nodes theo Ä‘á»™ liÃªn quan
    all_nodes.sort(key=lambda x: x.score if hasattr(x, 'score') else 0, reverse=True)
    
    # Láº¥y top 5 nodes
    top_nodes = all_nodes[:5]
    
    # Chuáº©n bá»‹ context cho LLM
    context = "\n\n".join([
        f"Äoáº¡n vÄƒn tá»« truyá»‡n '{node.node.metadata.get('story_name', 'Unknown')}':\n{node.node.text}"
        for node in top_nodes
    ])
    
    # Gá»i LLM Ä‘á»ƒ tá»•ng há»£p cÃ¢u tráº£ lá»i
    response = llm.complete(PROMPT.format(user_input=user_input, context=context))
    answer = response.text
    
    # Format káº¿t quáº£
    
    result = f"=================="
    result += f"\n\n      DEBUG\n"
    result += f"=================="
    result += "ğŸ“– CÃ¡c Ä‘oáº¡n vÄƒn tham kháº£o:\n\n"
    for i, node in enumerate(top_nodes, 1):
        result += f"{i}. Äoáº¡n vÄƒn tá»« truyá»‡n '{node.node.metadata.get('story_name', 'Unknown')}':\n"
        result += f"   {node.node.text[:200]}...\n"
        if hasattr(node, 'score'):
            result += f"   Äá»™ liÃªn quan: {node.score:.2f}\n\n"

    result += f"\n\n=================="
    result += f"\n\n  FINAL RESPONSE\n"
    result += f"==================\n"
    result += f"ğŸ“š CÃ¢u tráº£ lá»i:\n{answer}\n\n"
    
    return result


def create_gradio_interface(story_retrievers, examples, llm):
    # Táº¡o danh sÃ¡ch truyá»‡n cho dropdown
    story_options = ["Táº¥t cáº£ truyá»‡n"] + list(story_retrievers.keys())
    
    with gr.Blocks(title="Story RAG Chatbot") as interface:
        gr.Markdown("# ğŸ“š Story RAG Chatbot")
        gr.Markdown("Há»i Ä‘Ã¡p vá» ná»™i dung cÃ¡c truyá»‡n Ä‘Ã£ Ä‘Æ°á»£c index.")
        
        with gr.Row():
            with gr.Column():
                story_dropdown = gr.Dropdown(
                    label="Chá»n truyá»‡n",
                    choices=story_options,
                    value="Táº¥t cáº£ truyá»‡n"
                )
                user_input = gr.Textbox(
                    label="CÃ¢u há»i cá»§a báº¡n",
                    placeholder="Nháº­p cÃ¢u há»i cá»§a báº¡n á»Ÿ Ä‘Ã¢y...",
                    lines=3
                )
                submit_btn = gr.Button("Gá»­i cÃ¢u há»i")
            
            with gr.Column():
                output = gr.Textbox(
                    label="Káº¿t quáº£",
                    lines=10,
                    interactive=False
                )
        
        # ThÃªm examples
        gr.Examples(
            examples=examples,
            inputs=user_input
        )
        
        submit_btn.click(
            fn=lambda x, y: process_query(x, story_retrievers, y, llm),
            inputs=[user_input, story_dropdown],
            outputs=output
        )
        
        user_input.submit(
            fn=lambda x, y: process_query(x, story_retrievers, y, llm),
            inputs=[user_input, story_dropdown],
            outputs=output
        )
    
    return interface


def main():
    # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a cÃ¡c file truyá»‡n
    books_dir = Path("books")
    
    print(f"ğŸš€ Äang Ä‘á»c vÃ  phÃ¢n tÃ­ch cÃ¡c file truyá»‡n tá»«: {books_dir}")
    story_documents, story_nodes = load_and_parse_documents(books_dir)
    
    print(f"ğŸ“„ ÄÃ£ táº¡o {sum(len(nodes) for nodes in story_nodes.values())} node tá»« {len(story_nodes)} truyá»‡n.")
    
    print("ğŸ” Äang xÃ¢y dá»±ng hoáº·c táº£i há»‡ thá»‘ng chá»‰ má»¥c dáº¡ng Ä‘á»“ thá»‹...")
    graph, story_indices, story_retrievers = build_or_load_graph_index(story_documents, story_nodes)
    
    print("ğŸŒ Äang khá»Ÿi Ä‘á»™ng giao diá»‡n web...")
    interface = create_gradio_interface(story_retrievers, EXAMPLES, llm)
    interface.launch(share=False)  # Chá»‰ má»Ÿ local server

if __name__ == "__main__":
    main()